{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7019145,
     "sourceType": "datasetVersion",
     "datasetId": 4036075
    },
    {
     "sourceId": 7019161,
     "sourceType": "datasetVersion",
     "datasetId": 4036087
    }
   ],
   "dockerImageVersionId": 30588,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:47.699285Z",
     "iopub.execute_input": "2023-11-26T02:50:47.699727Z",
     "iopub.status.idle": "2023-11-26T02:50:47.707450Z",
     "shell.execute_reply.started": "2023-11-26T02:50:47.699689Z",
     "shell.execute_reply": "2023-11-26T02:50:47.706561Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 超参数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 50\n",
    "batch_size = 16\n",
    "save_steps = 10\n",
    "num_workers = 4\n",
    "lr = 0.001\n",
    "lr_step = 40\n",
    "\n",
    "alpha = 0.7\n",
    "temperature = 7"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:47.924120Z",
     "iopub.execute_input": "2023-11-26T02:50:47.924537Z",
     "iopub.status.idle": "2023-11-26T02:50:47.929483Z",
     "shell.execute_reply.started": "2023-11-26T02:50:47.924508Z",
     "shell.execute_reply": "2023-11-26T02:50:47.928555Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def read_split_data(root, val_rate=0.2):\n",
    "    \n",
    "    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root)\n",
    "\n",
    "    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]\n",
    "    flower_class.sort()\n",
    "    class_indices = dict((k, v) for v, k in enumerate(flower_class))\n",
    "    print(class_indices) \n",
    "\n",
    "    train_images_path = []\n",
    "    train_images_label = []\n",
    "    val_images_path = []\n",
    "    val_images_label = []\n",
    "    every_class_num = []\n",
    "   \n",
    "    for cla in flower_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)]\n",
    "        images.sort()\n",
    "        image_class = class_indices[cla]\n",
    "        \n",
    "        every_class_num.append(len(images))\n",
    "        val_path = random.sample(images, k=int(len(images) * val_rate))\n",
    "        \n",
    "        # 划分训练集 和 验证集\n",
    "        for img_path in images:\n",
    "            if img_path in val_path:  \n",
    "                val_images_path.append(img_path)\n",
    "                val_images_label.append(image_class)\n",
    "            else:  \n",
    "                train_images_path.append(img_path)\n",
    "                train_images_label.append(image_class)\n",
    "\n",
    "    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n",
    "    print(\"{} images for training.\".format(len(train_images_path)))\n",
    "    print(\"{} images for validation.\".format(len(val_images_path)))\n",
    "    assert len(train_images_path) > 0, \"number of training images must greater than 0.\"\n",
    "    assert len(val_images_path) > 0, \"number of validation images must greater than 0.\"\n",
    "\n",
    "    return train_images_path, train_images_label, val_images_path, val_images_label\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, images_path, images_class, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.images_class = images_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return torch.as_tensor(img), torch.as_tensor(label)\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/flowers/flower_photos\"\n",
    "train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_root)\n",
    "\n",
    "# =============================== Transform ===============================\n",
    "img_size = 224\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(img_size),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize(int(img_size * 1.143)),\n",
    "                                      transforms.CenterCrop(img_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "# =============================== DataSet ===============================\n",
    "\n",
    "train_dataset = MyDataSet(images_path=train_images_path,\n",
    "                          images_class=train_images_label,\n",
    "                          transform=train_transform)\n",
    "\n",
    "val_dataset = MyDataSet(images_path=val_images_path,\n",
    "                        images_class=val_images_label,\n",
    "                        transform=val_transform)\n",
    "\n",
    "# =============================== DataLoader ===============================\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=num_workers)\n",
    "\n",
    "print(\"len(train_loader) = {}\".format(len(train_loader)))\n",
    "print(\"len(val_loader) = {}\".format(len(val_loader)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:48.119884Z",
     "iopub.execute_input": "2023-11-26T02:50:48.120278Z",
     "iopub.status.idle": "2023-11-26T02:50:48.170459Z",
     "shell.execute_reply.started": "2023-11-26T02:50:48.120245Z",
     "shell.execute_reply": "2023-11-26T02:50:48.169533Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n3670 images were found in the dataset.\n2939 images for training.\n731 images for validation.\nlen(train_loader) = 184\nlen(val_loader) = 46\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class RunningAverage():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.loss_sum = 0\n",
    "        self.acc_sum = 0\n",
    "    \n",
    "    def update(self, loss, acc):\n",
    "        self.loss_sum += loss\n",
    "        self.acc_sum += acc\n",
    "        self.steps += 1\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.loss_sum/float(self.steps), self.acc_sum/float(self.steps)\n",
    "\n",
    "\n",
    "\n",
    "def kd_train_and_evaluate(teacher_model, student_model, train_dataloader, val_dataloader, criteria, optimizer, scheduler, alpha, temperature):\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "\n",
    "        # ---------- train ------------\n",
    "        \n",
    "        student_model.train()\n",
    "        metric_avg = RunningAverage()\n",
    "        \n",
    "        for i, (train_batch, labels_batch) in enumerate(train_dataloader):\n",
    "            train_batch, labels_batch = train_batch.to(device), labels_batch.to(device)\n",
    "            \n",
    "            student_outputs = student_model(train_batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(train_batch)\n",
    "                \n",
    "            loss = criteria(student_outputs, teacher_outputs, labels_batch, alpha, temperature)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % save_steps == 0:\n",
    "                student_outputs = student_outputs.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                predict_labels = np.argmax(student_outputs, axis=1)\n",
    "                acc = np.sum(predict_labels == labels_batch) / float(labels_batch.size)\n",
    "                metric_avg.update(loss.item(), acc)\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_loss, train_acc = metric_avg()\n",
    "        print(\"- Train metrics: loss={}, acc={}\".format(train_loss, train_acc))      \n",
    "\n",
    "        # ---------- validate ------------\n",
    "\n",
    "        student_model.eval()\n",
    "        metric_avg = RunningAverage()\n",
    "\n",
    "        for val_batch, labels_batch in val_dataloader:\n",
    "            val_batch, labels_batch = val_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "            student_outputs = student_model(val_batch)\n",
    "            loss = 0\n",
    "            \n",
    "            student_outputs = student_outputs.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            predict_labels = np.argmax(student_outputs, axis=1)\n",
    "            acc = np.sum(predict_labels == labels_batch) / float(labels_batch.size)\n",
    "            metric_avg.update(loss, acc)\n",
    "        _, val_acc = metric_avg()\n",
    "        print(\"- Validate metrics: acc={}\".format(val_acc))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:48.205684Z",
     "iopub.execute_input": "2023-11-26T02:50:48.206002Z",
     "iopub.status.idle": "2023-11-26T02:50:48.221895Z",
     "shell.execute_reply.started": "2023-11-26T02:50:48.205975Z",
     "shell.execute_reply": "2023-11-26T02:50:48.220830Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 教师网络"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "teacher_model = nn.Sequential(models.resnet50(),\n",
    "                              nn.Dropout(0.5),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(in_features=1000, out_features=5, bias=True))\n",
    "\n",
    "checkpoint = torch.load('/kaggle/input/resnet-50-weights/best.pth')\n",
    "teacher_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "teacher_model.to(device)\n",
    "\n",
    "print(\"prepare the teacher model --- done\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:48.340861Z",
     "iopub.execute_input": "2023-11-26T02:50:48.341611Z",
     "iopub.status.idle": "2023-11-26T02:50:49.027102Z",
     "shell.execute_reply.started": "2023-11-26T02:50:48.341574Z",
     "shell.execute_reply": "2023-11-26T02:50:49.026050Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "prepare the teacher model --- done\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 学生网络"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "student_model = nn.Sequential(models.resnet34(),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.5),\n",
    "                         nn.Linear(in_features=1000, out_features=5, bias=True))\n",
    "\n",
    "student_model.to(device)\n",
    "print(\"prepare the student model --- done\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:49.028997Z",
     "iopub.execute_input": "2023-11-26T02:50:49.029328Z",
     "iopub.status.idle": "2023-11-26T02:50:49.360666Z",
     "shell.execute_reply.started": "2023-11-26T02:50:49.029302Z",
     "shell.execute_reply": "2023-11-26T02:50:49.359727Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "prepare the student model --- done\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 损失函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def loss_fn_kd(student_outputs, teacher_outputs, labels, alpha, temperature):\n",
    "\n",
    "    alpha = alpha\n",
    "    T = temperature\n",
    "\n",
    "    KD_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(student_outputs/T, dim=1),\n",
    "                                                  F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
    "              F.cross_entropy(student_outputs, labels) * (1.- alpha)\n",
    "\n",
    "    return KD_loss"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:49.361782Z",
     "iopub.execute_input": "2023-11-26T02:50:49.362060Z",
     "iopub.status.idle": "2023-11-26T02:50:49.368058Z",
     "shell.execute_reply.started": "2023-11-26T02:50:49.362035Z",
     "shell.execute_reply": "2023-11-26T02:50:49.367187Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 优化器 与 学习率调度器"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=lr_step, gamma=0.1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:49.369372Z",
     "iopub.execute_input": "2023-11-26T02:50:49.369720Z",
     "iopub.status.idle": "2023-11-26T02:50:49.383513Z",
     "shell.execute_reply.started": "2023-11-26T02:50:49.369685Z",
     "shell.execute_reply": "2023-11-26T02:50:49.382682Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 开始训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "kd_train_and_evaluate(teacher_model, student_model, train_loader, val_loader, loss_fn_kd, optimizer, scheduler, alpha, temperature)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-26T02:50:49.385620Z",
     "iopub.execute_input": "2023-11-26T02:50:49.385980Z",
     "iopub.status.idle": "2023-11-26T03:03:54.407463Z",
     "shell.execute_reply.started": "2023-11-26T02:50:49.385954Z",
     "shell.execute_reply": "2023-11-26T03:03:54.406228Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/50\n- Train metrics: loss=9.806779359516344, acc=0.4473684210526316\n- Validate metrics: acc=0.5328557312252965\nEpoch 2/50\n- Train metrics: loss=8.057040189441881, acc=0.5657894736842105\n- Validate metrics: acc=0.5926383399209486\nEpoch 3/50\n- Train metrics: loss=7.9696633941248844, acc=0.5657894736842105\n- Validate metrics: acc=0.6131422924901185\nEpoch 4/50\n- Train metrics: loss=7.228650682850888, acc=0.5921052631578947\n- Validate metrics: acc=0.6904644268774703\nEpoch 5/50\n- Train metrics: loss=7.567201990830271, acc=0.5921052631578947\n- Validate metrics: acc=0.6957756916996047\nEpoch 6/50\n- Train metrics: loss=6.20396812338578, acc=0.6578947368421053\n- Validate metrics: acc=0.6341403162055336\nEpoch 7/50\n- Train metrics: loss=5.5960291310360555, acc=0.6447368421052632\n- Validate metrics: acc=0.7120800395256918\nEpoch 8/50\n- Train metrics: loss=5.658480305420725, acc=0.6710526315789473\n- Validate metrics: acc=0.7251729249011858\nEpoch 9/50\n- Train metrics: loss=6.2022033114182324, acc=0.6118421052631579\n- Validate metrics: acc=0.7698863636363636\nEpoch 10/50\n- Train metrics: loss=5.173097409700093, acc=0.6480263157894737\n- Validate metrics: acc=0.6794713438735177\nEpoch 11/50\n- Train metrics: loss=4.827453387411017, acc=0.7105263157894737\n- Validate metrics: acc=0.7454298418972332\nEpoch 12/50\n- Train metrics: loss=4.294722030037327, acc=0.7105263157894737\n- Validate metrics: acc=0.760251976284585\nEpoch 13/50\n- Train metrics: loss=4.08494254162437, acc=0.7269736842105263\n- Validate metrics: acc=0.8038537549407114\nEpoch 14/50\n- Train metrics: loss=4.105724510393645, acc=0.7368421052631579\n- Validate metrics: acc=0.7982954545454545\nEpoch 15/50\n- Train metrics: loss=3.8064906346170524, acc=0.7664473684210527\n- Validate metrics: acc=0.7672924901185771\nEpoch 16/50\n- Train metrics: loss=4.056986181359542, acc=0.75\n- Validate metrics: acc=0.7978013833992094\nEpoch 17/50\n- Train metrics: loss=4.036394652567412, acc=0.7138157894736842\n- Validate metrics: acc=0.8154644268774703\nEpoch 18/50\n- Train metrics: loss=4.427856432764154, acc=0.7368421052631579\n- Validate metrics: acc=0.83300395256917\nEpoch 19/50\n- Train metrics: loss=4.021938135749416, acc=0.7302631578947368\n- Validate metrics: acc=0.8094120553359684\nEpoch 20/50\n- Train metrics: loss=4.207504159525821, acc=0.7203947368421053\n- Validate metrics: acc=0.849308300395257\nEpoch 21/50\n- Train metrics: loss=3.118342951724404, acc=0.805921052631579\n- Validate metrics: acc=0.7861907114624506\nEpoch 22/50\n- Train metrics: loss=3.1509660858857003, acc=0.7927631578947368\n- Validate metrics: acc=0.8376976284584979\nEpoch 23/50\n- Train metrics: loss=3.0937113259968005, acc=0.7960526315789473\n- Validate metrics: acc=0.8391798418972332\nEpoch 24/50\n- Train metrics: loss=3.295330336219386, acc=0.7993421052631579\n- Validate metrics: acc=0.8139822134387352\nEpoch 25/50\n- Train metrics: loss=2.841351678496913, acc=0.7993421052631579\n- Validate metrics: acc=0.8189229249011858\nEpoch 26/50\n- Train metrics: loss=3.003940538356179, acc=0.8223684210526315\n- Validate metrics: acc=0.8486907114624506\nEpoch 27/50\n- Train metrics: loss=2.8784143046328894, acc=0.7796052631578947\n- Validate metrics: acc=0.8547430830039526\nEpoch 28/50\n- Train metrics: loss=2.6238422268315365, acc=0.7664473684210527\n- Validate metrics: acc=0.8411561264822135\nEpoch 29/50\n- Train metrics: loss=2.9493388878671745, acc=0.7861842105263158\n- Validate metrics: acc=0.8527667984189723\nEpoch 30/50\n- Train metrics: loss=2.458291430222361, acc=0.8223684210526315\n- Validate metrics: acc=0.8390563241106719\nEpoch 31/50\n- Train metrics: loss=2.8298883563593815, acc=0.7598684210526315\n- Validate metrics: acc=0.879199604743083\nEpoch 32/50\n- Train metrics: loss=2.470739395994889, acc=0.7993421052631579\n- Validate metrics: acc=0.8404150197628458\nEpoch 33/50\n- Train metrics: loss=2.5199940016395166, acc=0.8256578947368421\n- Validate metrics: acc=0.8588191699604744\nEpoch 34/50\n- Train metrics: loss=2.651653791728773, acc=0.8552631578947368\n- Validate metrics: acc=0.856842885375494\nEpoch 35/50\n- Train metrics: loss=2.4194398679231344, acc=0.8552631578947368\n- Validate metrics: acc=0.8514081027667985\nEpoch 36/50\n- Train metrics: loss=2.570391868290148, acc=0.8256578947368421\n- Validate metrics: acc=0.8561017786561266\nEpoch 37/50\n- Train metrics: loss=2.266064342699553, acc=0.7993421052631579\n- Validate metrics: acc=0.8615365612648221\nEpoch 38/50\n- Train metrics: loss=2.608907053345128, acc=0.8125\n- Validate metrics: acc=0.8859930830039526\nEpoch 39/50\n- Train metrics: loss=2.408629160178335, acc=0.7730263157894737\n- Validate metrics: acc=0.8777173913043478\nEpoch 40/50\n- Train metrics: loss=2.30896784757313, acc=0.805921052631579\n- Validate metrics: acc=0.8853754940711462\nEpoch 41/50\n- Train metrics: loss=2.0333423175309835, acc=0.8157894736842105\n- Validate metrics: acc=0.8995800395256918\nEpoch 42/50\n- Train metrics: loss=1.8093048396863436, acc=0.8914473684210527\n- Validate metrics: acc=0.9009387351778656\nEpoch 43/50\n- Train metrics: loss=1.8658612841054012, acc=0.8322368421052632\n- Validate metrics: acc=0.8982213438735178\nEpoch 44/50\n- Train metrics: loss=1.7023116601140875, acc=0.8651315789473685\n- Validate metrics: acc=0.90625\nEpoch 45/50\n- Train metrics: loss=1.7601261923187657, acc=0.8618421052631579\n- Validate metrics: acc=0.9042737154150197\nEpoch 46/50\n- Train metrics: loss=1.5574258660015308, acc=0.8881578947368421\n- Validate metrics: acc=0.9036561264822135\nEpoch 47/50\n- Train metrics: loss=1.6405154717595953, acc=0.8421052631578947\n- Validate metrics: acc=0.9069911067193676\nEpoch 48/50\n- Train metrics: loss=1.6951475017949154, acc=0.8552631578947368\n- Validate metrics: acc=0.9036561264822135\nEpoch 49/50\n- Train metrics: loss=1.8113635621572797, acc=0.8980263157894737\n- Validate metrics: acc=0.9063735177865613\nEpoch 50/50\n- Train metrics: loss=1.6964462117144936, acc=0.8651315789473685\n- Validate metrics: acc=0.9029150197628458\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
